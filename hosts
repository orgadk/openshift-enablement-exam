# This is an example of a bring your own (byo) host inventory

# Create an OSEv3 group that contains the masters and nodes groups
[OSEv3:children]
masters
nodes
etcd
#nfs
#glusterfs


# Set variables common for all OSEv3 hosts
[OSEv3:vars]
# SSH user, this user should allow ssh based auth without requiring a
# password. If using ssh key based auth, then the key should be managed by an
# ssh agent.
ansible_ssh_user=BASTION_USERNAME

# If ansible_ssh_user is not root, ansible_become must be set to true and the
# user must be configured for passwordless sudo
ansible_become=yes

# Debug level for all OpenShift components (Defaults to 2)
debug_level=2

# deployment type valid values are origin, online, atomic-enterprise, and openshift-enterprise
deployment_type=openshift-enterprise

# Specify the generic release of OpenShift to install. This is used mainly just during installation, after which we
# rely on the version running on the first master. Works best for containerized installs where we can usually
# use this to lookup the latest exact version of the container images, which is the tag actually used to configure
# the cluster. For RPM installations we just verify the version detected in your configured repos matches this
# release.
openshift_release=v3.7

# Docker Configuration
# Add additional, insecure, and blocked registries to global docker configuration
# For enterprise deployment types we ensure that registry.access.redhat.com is
# included if you do not include it
openshift_docker_options="--log-driver=json-file --log-opt max-size=50m --log-opt max-file=100 --storage-opt dm.basesize=15G"

# htpasswd auth
openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}]
# Defining htpasswd users
openshift_master_htpasswd_users={'admin': '$apr1$7aiANAYb$TOUYVUqnBqBlD5AQEIMYw1'} 


osm_default_node_selector='region=primary'


# GCE
openshift_cloudprovider_kind=gce

# Configure additional projects
#openshift_additional_projects={'my-project': {'default_node_selector': 'label=value'}}

# Enable cockpit
osm_use_cockpit=true
#
# Set cockpit plugins
osm_cockpit_plugins=['cockpit-kubernetes']

# Native high availability cluster method with optional load balancer.
# If no lb group is defined, the installer assumes that a load balancer has
# been preconfigured. For installation the value of
# openshift_master_cluster_hostname must resolve to the load balancer
# or to one or all of the masters defined in the inventory if no load
# balancer is present.
openshift_master_cluster_method=native
openshift_master_cluster_hostname=INTERNAL_MASTER
openshift_master_cluster_public_hostname=EXTERNAL_MASTER

# default subdomain to use for exposed routes
openshift_master_default_subdomain=APP_SUBDOMAIN

# OpenShift Router Options
#
# An OpenShift router will be created during install if there are
# nodes present with labels matching the default router selector,
# "region=infra". Set openshift_node_labels per node as needed in
# order to label nodes.
#
openshift_hosted_router_selector='region=infra'
openshift_hosted_manage_router=true

# Openshift Registry Options
#
# An OpenShift registry will be created during install if there are
# nodes present with labels matching the default registry selector,
# "region=infra". Set openshift_node_labels per node as needed in
# order to label nodes.
#
openshift_hosted_registry_selector='region=infra'
openshift_hosted_manage_registry=true
#openshift_hosted_registry_storage_kind=glusterfs

# Metrics deployment
# See: https://docs.openshift.com/enterprise/latest/install_config/cluster_metrics.html
#
# By default metrics are not automatically deployed, set this to enable them
openshift_metrics_install_metrics=true
#openshift_metrics_cassandra_storage_type=dynamic
openshift_metrics_duration=1
openshift_metrics_hawkular_replicas=2
openshift_metrics_cassandra_replicas=3
openshift_metrics_hawkular_nodeselector='{"region":"infra"}'
openshift_metrics_cassandra_nodeselector='{"region":"infra"}'
openshift_metrics_heapster_nodeselector='{"region":"infra"}'
openshift_metrics_selector="region=infra"
openshift_metrics_install_hawkular_agent=true


# Logging deployment
#
# Currently logging deployment is disabled by default, enable it by setting this
openshift_logging_install_logging=true
#openshift_logging_es_pvc_dynamic=true
openshift_logging_es_pvc_size= 10Gi
openshift_logging_es_cluster_size= 3
openshift_logging_es_number_of_replicas=2
openshift_logging_kibana_replica_count=2
openshift_logging_es_nodeselector='{"region":"infra"}'
openshift_logging_kibana_nodeselector='{"region":"infra"}'
openshift_logging_curator_nodeselector='{"region":"infra"}'
openshift_logging_curator_default_days=2
#events routers
openshift_logging_fluentd_audit_container_engine=true
openshift_logging_install_eventrouter=true

# Configure the multi-tenant SDN plugin (default is 'redhat/openshift-ovs-subnet')
#os_sdn_network_plugin_name='redhat/openshift-ovs-multitenant'
os_sdn_network_plugin_name='redhat/openshift-ovs-networkpolicy'


# Disable the OpenShift SDN plugin
# openshift_use_openshift_sdn=False

# Configure SDN cluster network and kubernetes service CIDR blocks. These
# network blocks should be private and should not conflict with network blocks
# in your infrastructure that pods may require access to. Can not be changed
# after deployment.
osm_cluster_network_cidr=10.1.0.0/16
#openshift_portal_net=172.30.0.0/16


# ExternalIPNetworkCIDRs controls what values are acceptable for the
# service external IP field. If empty, no externalIP may be set. It
# may contain a list of CIDRs which are checked for access. If a CIDR
# is prefixed with !, IPs in that CIDR will be rejected. Rejections
# will be applied first, then the IP checked against one of the
# allowed CIDRs. You should ensure this range does not overlap with
# your nodes, pods, or service CIDRs for security reasons.
#openshift_master_external_ip_network_cidrs=['0.0.0.0/0']

# Configure number of bits to allocate to each hostâ€™s subnet e.g. 8
# would mean a /24 network on the host.
#osm_host_subnet_length=8

# Configure dnsIP in the node config
#openshift_dns_ip=172.30.0.1

#service broker settings

openshift_enable_service_catalog=true
openshift_template_service_broker_namespaces=['openshift']
#openshift_hosted_etcd_storage_kind=glusterfs


#cloudforms settings
openshift_cfme_install_app=true
openshift_management_install_beta=true
openshift_management_app_template=cfme-template
openshift_management_storage_class=cloudprovider

# checks
openshift_disable_check=docker_storage,memory_availability,disk_availability,docker_image_availability,docker_storage_driver,package_version,package_availability

#crio
#openshift_use_crio=true
#openshift_crio_enable_docker_gc=true

#openshift_storage_glusterfs_namespace=glusterfs 
#openshift_storage_glusterfs_name=storage
#openshift_storage_glusterfs_storageclass=true
#openshift_storage_glusterfs_block_deploy=true
#openshift_storage_glusterfs_block_host_vol_create=true
#openshift_storage_glusterfs_s3_deploy=true
#openshift_storage_glusterfs_heketi_admin_key=admin
#openshift_storage_glusterfs_heketi_user_key=user




#prometheus
openshift_hosted_prometheus_deploy=true
#openshift_prometheus_storage_kind=glusterfs
openshift_prometheus_namespace=openshift-metrics
openshift_prometheus_node_selector={"region":"infra"}
openshift_prometheus_storage_type= pvc
openshift_prometheus_alertmanager_storage_type= pvc
openshift_prometheus_alterbuffer_storage_type= pvc
openshift_prometheus_pvc_name= prometheus-data
openshift_prometheus_alertmanager_pvc_name= alertmanager-data
openshift_prometheus_alterbuffer_pvc_name= alertbuffer-data
openshift_prometheus_pvc_size= 3G
openshift_prometheus_alertmanager_pvc_size= 1G
openshift_prometheus_alertbuffer_pvc_size= 2G
openshift_prometheus_pvc_access_modes= [ReadWriteOnce]

# audit

openshift_master_audit_config={"enabled": true, "auditFilePath": "/var/log/ocp-audit.log", "maximumFileRetentionDays": 2, "maximumFileSizeMegabytes": 50, "maximumRetainedFiles": 5}


#alpha features
osm_api_server_args={ "feature-gates": [ "PersistentLocalVolumes=true", "AdvancedAuditing=true" ] }
osm_controller_args={ "feature-gates": [ "PersistentLocalVolumes=true" ] }


# host group for masters
[masters]
master[1:3]

[etcd]
master[1:3]

#[nfs]
#ose-bastion

# NOTE: Currently we require that masters be part of the SDN which requires that they also be nodes
# However, in order to ensure that your masters are not burdened with running pods you should
# make them unschedulable by adding openshift_schedulable=False any node that's also a master.
[nodes]
master[1:3] openshift_schedulable=false openshift_node_labels="{'region': 'primary'}" 
node[1:3] openshift_node_labels="{'region': 'primary'}" 
infranode[1:3] openshift_node_labels="{'region': 'infra'}"



#[glusterfs] 
#infranode[1:3]


